{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Molecular Feature Vector Construction\n",
    "\n",
    "For each molecule (node), create a feature vector by concatenating:\n",
    "- a) Molecular Fingerprint: This is already a high-dimensional vector.\n",
    "- b) Scalar Properties: Convert each scalar property (e.g., molecular weight, heavy atom count) into a one-hot or multi-hot encoding, or embed them into a higher-dimensional space.\n",
    "From the available columns, let's select the following features for our vector:\n",
    "\n",
    "Fingerprint (already a 2048-dimensional vector)\n",
    "mapped_protein_class (categorical)\n",
    "map_target_taxonomy (categorical)\n",
    "map_target_organism (categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading pre-calculated fingerprints\n",
      "INFO:__main__:Total fused fingerprints: 20\n",
      "Calculating fingerprints (mapc): 100%|██████████| 100/100 [00:00<00:00, 102.91it/s]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import tmap as tm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from faerun import Faerun\n",
    "from mhfp.encoder import MHFPEncoder\n",
    "from rdkit import Chem\n",
    "from mapchiral.mapchiral import encode as mapc_enc\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
    "import html\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import tmap as tm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from faerun import Faerun\n",
    "from mhfp.encoder import MHFPEncoder\n",
    "from rdkit import Chem\n",
    "from mapchiral.mapchiral import encode as mapc_enc\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
    "import html\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import tmap as tm\n",
    "from timeit import default_timer as timer\n",
    "from faerun import Faerun\n",
    "from mhfp.encoder import MHFPEncoder\n",
    "from rdkit import Chem\n",
    "from mapchiral.mapchiral import encode as mapc_enc\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
    "import html\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def encode_smiles(smiles, encoder):\n",
    "    \"\"\"\n",
    "    Helper function to encode a single SMILES string based on the specified encoder.\n",
    "\n",
    "    Args:\n",
    "    smiles (str): The SMILES string to encode.\n",
    "    encoder (str): The encoding method, 'mhfp' or 'mapc'.\n",
    "    kwargs: Additional arguments for the encoder.\n",
    "\n",
    "    Returns:\n",
    "    tm.VectorUint: Encoded fingerprint.\n",
    "    \"\"\"\n",
    "    if encoder == 'mapc':\n",
    "        return tm.VectorUint(mapc_enc(Chem.MolFromSmiles(smiles), max_radius=2, n_permutations=2048, mapping=False))\n",
    "    elif encoder == 'mhfp':\n",
    "        perm = 512\n",
    "        mhfp_enc = MHFPEncoder(perm)\n",
    "        return tm.VectorUint(mhfp_enc.encode(smiles))\n",
    "    return None\n",
    "\n",
    "def calculate_threshold(data):\n",
    "    # Function to calculate threshold using IQR method\n",
    "    q1, q3 = np.percentile(data, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    threshold = q3 + 1.5 * iqr\n",
    "    return threshold\n",
    "\n",
    "def calculate_molecular_properties(smiles):\n",
    "    \"\"\"\n",
    "    Calculate molecular properties using RDKit.\n",
    "    \n",
    "    Args:\n",
    "    smiles (str): SMILES string of the molecule.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (HAC, fraction_aromatic_atoms, number_of_rings, clogP, fraction_Csp3)\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    \n",
    "    hac = mol.GetNumHeavyAtoms()\n",
    "    num_aromatic_atoms = sum(1 for atom in mol.GetAtoms() if atom.GetIsAromatic())\n",
    "    fraction_aromatic_atoms = num_aromatic_atoms / hac if hac > 0 else 0\n",
    "    number_of_rings = rdMolDescriptors.CalcNumRings(mol)\n",
    "    clogP = Descriptors.MolLogP(mol)\n",
    "    fraction_Csp3 = Descriptors.FractionCSP3(mol)\n",
    "    \n",
    "    return (hac, fraction_aromatic_atoms, number_of_rings, clogP, fraction_Csp3)\n",
    "\n",
    "def calculate_fingerprints(df, encoder):\n",
    "    \"\"\"\n",
    "    Calculate fingerprints for SMILES strings in the dataframe.\n",
    "    \n",
    "    Args:\n",
    "    df (pandas.DataFrame): DataFrame containing 'canonical_smiles' column.\n",
    "    encoder (str): The encoder object used to encode SMILES strings ('mhfp' or 'mapc').\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (list of encoded fingerprints, list of valid indices).\n",
    "    \"\"\"\n",
    "    fingerprints = []\n",
    "    valid_indices = []\n",
    "\n",
    "    for idx, smiles in enumerate(tqdm(df['canonical_smiles'], desc=f\"Calculating fingerprints ({encoder})\")):\n",
    "        try:\n",
    "            with warnings.catch_warnings(record=True) as w:\n",
    "                warnings.simplefilter(\"always\")\n",
    "                enc_fingerprint = encode_smiles(smiles, encoder)\n",
    "                if w and issubclass(w[-1].category, UserWarning):\n",
    "                    logger.warning(f\"Warning for Fingerprints {smiles}: {str(w[-1].message)}\")\n",
    "                    continue\n",
    "            fingerprints.append(enc_fingerprint)\n",
    "            valid_indices.append(idx)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error encoding Fingerprints {smiles}: {str(e)}\")\n",
    "        \n",
    "    return fingerprints, valid_indices\n",
    "\n",
    "def list_to_vectorUint(lst):\n",
    "    \"\"\"\n",
    "    Convert list or numpy array to tm.VectorUint type.\n",
    "    \n",
    "    Args:\n",
    "    lst (list or np.array): The list or array to convert.\n",
    "    \n",
    "    Returns:\n",
    "    tm.VectorUint: Converted VectorUint object.\n",
    "    \"\"\"\n",
    "    return tm.VectorUint(lst)\n",
    "\n",
    "def safe_create_categories(series):\n",
    "    \"\"\"\n",
    "    Create categories from a pandas Series, handling NaN values.\n",
    "    \n",
    "    Args:\n",
    "    series (pandas.Series): The input data series.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (labels, data) for Faerun plotting.\n",
    "    \"\"\"\n",
    "    return Faerun.create_categories(series.fillna('Unknown').astype(str))\n",
    "\n",
    "def map_protein_class(value):\n",
    "    \"\"\"Map protein class to a simplified category.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return str('NaN')\n",
    "        \n",
    "    if 'enzyme' in value:\n",
    "            return 'Enzyme'\n",
    "    elif 'membrane receptor' in value: \n",
    "        return 'Membrane receptor'\n",
    "    elif ' ion channel' in value:\n",
    "        return 'Ion Channel'\n",
    "    elif 'transcription factor' in value: \n",
    "        return 'Transcription factor'\n",
    "    elif 'epigenetic regulator' in value:\n",
    "        return 'Epigenetic regulator'\n",
    "    elif 'cytosolic protein' in value:\n",
    "        return 'cytosolic protein'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "def map_target_taxonomy(value):\n",
    "\n",
    "    if 'Eukaryotes' in value:\n",
    "        if 'Oxidoreductase' in value:\n",
    "            return 'Oxidoreductase'\n",
    "        elif 'Transferase' in value:\n",
    "            return 'Transferase' \n",
    "        elif 'Hydrolase' in value:\n",
    "            return 'Hydrolase'\n",
    "        else:\n",
    "            return 'Eukaryotes'\n",
    "    elif 'Bacteria' in value: \n",
    "        return 'Bacteria'\n",
    "    elif 'Fungi' in value:\n",
    "        return 'Fungi'\n",
    "    elif 'Viruses' in value: \n",
    "        return 'Viruses'\n",
    "    elif 'unclassified' in value:\n",
    "        return 'Unclassified'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "def map_target_organism(value):\n",
    "\n",
    "    \"\"\"Map target organism to a simplified category.\"\"\"\n",
    "    if 'sapiens' in value:\n",
    "        return 'Homo sapiens'\n",
    "    elif 'virus' in value:\n",
    "        return 'Virus'\n",
    "    elif any(organism in value for organism in ['rattus', 'Musculus']):\n",
    "        return 'Rat'\n",
    "    elif 'taurus' in value:\n",
    "        return 'Bovid'\n",
    "    elif any(organism in value for organism in ['scrofa', 'Macaca', 'porcellus', 'oryctolagus', 'canis', 'Cricetulus']):\n",
    "        return 'Other mammals'\n",
    "    elif any(bacteria in value for bacteria in ['Mycobacterium', 'Escherichia', 'Salmonella', 'Staphylococcus', 'Pseudomonas', 'Bacillus', 'Acinetobacter']):\n",
    "        return 'Bacteria'\n",
    "    elif any(parasite in value for parasite in ['Plasmodium', 'Trypanosoma', 'Schistosoma', 'Leishmania']):\n",
    "        return 'Parasites'\n",
    "    else:\n",
    "        return 'Others'\n",
    "\n",
    "def select_value(group):\n",
    "    \"\"\"Select the correct value based on the 'standard_type' category.\"\"\"\n",
    "    greater_value_terms = ['Activity', 'Inhibition', 'Potency', '% Inhibition', 'Percent Effect']\n",
    "    \n",
    "    # Check if the standard_type is in the greater_value_terms\n",
    "    if group['standard_type'].iloc[0] in greater_value_terms:\n",
    "        # If the standard_type is in the list, select the row with the maximum standard_value\n",
    "        return group.loc[group['standard_value'].idxmax()]\n",
    "    else:\n",
    "        # Otherwise, select the row with the minimum standard_value\n",
    "        return group.loc[group['standard_value'].idxmin()]\n",
    "    \n",
    "def minhash_fingerprints(df, fingerprints, valid_indices):\n",
    "    \"\"\"\n",
    "    Combine fingerprints by taking the minimum values for identical targets.\n",
    "    \n",
    "    Args:\n",
    "    df (pandas.DataFrame): The DataFrame with target IDs.\n",
    "    fingerprints (list): List of tm.VectorUint fingerprints.\n",
    "    valid_indices (list): Indices of valid entries in the original DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.Series: Combined fingerprints as tm.VectorUint.\n",
    "    \"\"\"\n",
    "    df_processed = df.loc[valid_indices].copy()\n",
    "    df_processed['fingerprint_vector'] = pd.Series(fingerprints, index=valid_indices)\n",
    "    df_processed['fingerprint_vector'] = df_processed['fingerprint_vector'].apply(np.array)\n",
    "\n",
    "    result = df_processed.groupby('Target_ID').agg({\n",
    "        'fingerprint_vector': lambda x: np.min(np.vstack(x), axis=0).tolist(),\n",
    "        **{col: 'first' for col in df_processed.columns if col not in ['fingerprint_vector', 'Target_ID']}\n",
    "    }).reset_index()\n",
    "    \n",
    "    return result['fingerprint_vector']\n",
    "\n",
    "def plot_faerun(x, y, s, t, df):\n",
    "\n",
    "    \"\"\"\n",
    "    Plot the data using Faerun.\n",
    "    \n",
    "    Args:\n",
    "    x (list): X coordinates.\n",
    "    y (list): Y coordinates.\n",
    "    s (list): Source nodes for tree plot.\n",
    "    t (list): Target nodes for tree plot.\n",
    "    df (pandas.DataFrame): DataFrame with target data.\n",
    "    \"\"\"\n",
    "    f = Faerun(view=\"front\", coords=False, clear_color=\"#FFFFFF\")\n",
    "\n",
    "    # Create categories\n",
    "    protein_class_labels, protein_class_data = safe_create_categories(df['mapped_protein_class'])\n",
    "    taxonomy_labels, taxonomy_data = safe_create_categories(df['map_target_taxonomy'])\n",
    "    organism_labels, organism_data = safe_create_categories(df['map_target_organism'])\n",
    "\n",
    "    labels = []\n",
    "    hac_data = []\n",
    "    frac_aromatic_data = []\n",
    "    num_rings_data = []\n",
    "    clogp_data = []\n",
    "    frac_csp3_data = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        target_name = str(row[\"target_name\"]).strip()  # Convert to string and remove leading/trailing whitespace\n",
    "        target_name = html.escape(target_name)  # Escape special characters\n",
    "        \n",
    "        if not target_name:\n",
    "            target_name = \"N/A\"  # Provide a default value if empty\n",
    "        \n",
    "        labels.append(\n",
    "            row['canonical_smiles']\n",
    "            + '__'\n",
    "            + f'<a target=\"_blank\" href=\"https://www.ebi.ac.uk/chembl/target_report_card/{row[\"Target_ID\"]}\">{target_name}</a><br>'\n",
    "        )\n",
    "        \n",
    "        # Calculate molecular properties\n",
    "        properties = calculate_molecular_properties(row['canonical_smiles'])\n",
    "        if properties:\n",
    "            hac, frac_aromatic, num_rings, clogp, frac_csp3 = properties\n",
    "            hac_data.append(hac)\n",
    "            frac_aromatic_data.append(frac_aromatic)\n",
    "            num_rings_data.append(num_rings)\n",
    "            clogp_data.append(clogp)\n",
    "            frac_csp3_data.append(frac_csp3)\n",
    "        else:\n",
    "            # Handle invalid SMILES\n",
    "            hac_data.append(None)\n",
    "            frac_aromatic_data.append(None)\n",
    "            num_rings_data.append(None)\n",
    "            clogp_data.append(None)\n",
    "            frac_csp3_data.append(None)\n",
    "   \n",
    "    # Calculate threshold for hac_data using IQR\n",
    "    hac_threshold = calculate_threshold(hac_data)\n",
    "    frac_threshold = calculate_threshold(frac_aromatic_data)\n",
    "    rings_threshold = calculate_threshold(num_rings_data)\n",
    "    clogp_threshold = calculate_threshold(clogp_data)\n",
    "    csp3_threshold = calculate_threshold(frac_csp3_data)\n",
    "\n",
    "    # Function to apply thresholds and return filtered data as separate lists\n",
    "    def apply_thresholds(hac_data, frac_aromatic_data, num_rings_data, clogp_data, frac_csp3_data):\n",
    "        filtered_hac = []\n",
    "        filtered_frac_aromatic = []\n",
    "        filtered_num_rings = []\n",
    "        filtered_clogp = []\n",
    "        filtered_frac_csp3 = []\n",
    "\n",
    "        # Iterate through all data points and apply thresholds\n",
    "        for hac, frac, rings, clogp, csp3 in zip(hac_data, frac_aromatic_data, num_rings_data, clogp_data, frac_csp3_data):\n",
    "            if hac <= hac_threshold and frac <= frac_threshold and rings <= rings_threshold and clogp <= clogp_threshold and csp3 <= csp3_threshold:\n",
    "                filtered_hac.append(hac)\n",
    "                filtered_frac_aromatic.append(frac)\n",
    "                filtered_num_rings.append(rings)\n",
    "                filtered_clogp.append(clogp)\n",
    "                filtered_frac_csp3.append(csp3)\n",
    "\n",
    "        return filtered_hac, filtered_frac_aromatic, filtered_num_rings, filtered_clogp, filtered_frac_csp3\n",
    "\n",
    "    filtered_hac,filtered_frac_aromatic, filtered_num_rings, filtered_clogp, filtered_frac_csp3 = apply_thresholds(hac_data, frac_aromatic_data, num_rings_data, clogp_data, frac_csp3_data)\n",
    "\n",
    "    # Add scatter plot\n",
    "    f.add_scatter(\n",
    "        \"mapc_targets\",\n",
    "        {\n",
    "            \"x\": x,\n",
    "            \"y\": y,\n",
    "            \"c\": [protein_class_data, taxonomy_data, organism_data, \n",
    "                  filtered_hac, filtered_frac_aromatic, filtered_num_rings, filtered_clogp, filtered_frac_csp3],\n",
    "            \"labels\": labels,\n",
    "        },\n",
    "        shader=\"smoothCircle\",\n",
    "        point_scale=4.0,\n",
    "        max_point_size=20,\n",
    "        interactive=True,\n",
    "        legend_labels=[protein_class_labels, taxonomy_labels, organism_labels],\n",
    "        categorical=[True, True, True, False, False, False, False, False],\n",
    "        colormap=['tab10', 'tab10', 'tab10', 'viridis', 'viridis', 'viridis', 'viridis', 'viridis'],\n",
    "        series_title=['Protein Class', 'Target Taxonomy', 'Target Organism',\n",
    "                      'HAC', 'Fraction Aromatic Atoms', 'Number of Rings', 'clogP', 'Fraction Csp3'],\n",
    "        has_legend=True,\n",
    "    )\n",
    "\n",
    "    # Add tree\n",
    "    f.add_tree(\"mapc_targets_tree\", {\"from\": s, \"to\": t}, point_helper=\"mapc_targets\", color=\"#222222\")\n",
    "    \n",
    "    # Plot\n",
    "    f.plot('mapc_targets', template='smiles')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Main\n",
    "csv_file = r\"C:\\Users\\biolab\\Desktop\\Alex\\Alex's\\OneDrive\\Work\\GNN-tmap-layout\\data\\dataset.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.head(100)\n",
    "\n",
    "# Define the path for saving/loading fingerprints\n",
    "fingerprints_file = Path(r\"C:\\Users\\biolab\\Desktop\\Alex\\Alex's\\OneDrive\\Work\\GNN-tmap-layout\\data\\fused_fingerprints.pkl\")\n",
    "\n",
    "if fingerprints_file.exists():\n",
    "    logger.info('Loading pre-calculated fingerprints')\n",
    "    with open(fingerprints_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    fused_fingerprints = data['fused_fingerprints']\n",
    "    logger.info(f'Total fused fingerprints: {len(fused_fingerprints)}')\n",
    "    valid_indices = data['valid_indices']\n",
    "    \n",
    "else:\n",
    "    logger.info('Calculating fingerprints')\n",
    "    # Calculate fingerprints\n",
    "    fingerprints, valid_indices = calculate_fingerprints(df, 'mapc')\n",
    "    # Combine fingerprints\n",
    "    logger.info('Combining fingerprints')\n",
    "    fused_fingerprints = minhash_fingerprints(df, fingerprints, valid_indices) \n",
    "    fused_fingerprints = fused_fingerprints.apply(list_to_vectorUint)\n",
    "    logger.info(f'Combination successful. Total fused fingerprints: {len(fused_fingerprints)}')\n",
    "    \n",
    "    # Save the calculated fingerprints and valid_indices\n",
    "    with open(fingerprints_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'fused_fingerprints': [list(fp) for fp in fused_fingerprints],\n",
    "            'valid_indices': valid_indices\n",
    "        }, f)\n",
    "    logger.info('Saved calculated fingerprints for future use')\n",
    "\n",
    "fingerprints, valid_indices = calculate_fingerprints(df, 'mapc')\n",
    "\n",
    "# Filter DataFrame\n",
    "df = df.iloc[valid_indices].reset_index(drop=True)\n",
    "df.insert(1, \"fingerprints\", fingerprints)\n",
    "df = df.groupby('Target_ID').apply(select_value).reset_index(drop=True) # group by Target ID and then value so the most active compound is the one that remains with the fused fp\n",
    "\n",
    "# Apply the mapping function to the column to reduce number of unique values and make it color codeable \n",
    "df['mapped_protein_class'] = df['target_protein_class'].apply(map_protein_class)\n",
    "df['map_target_taxonomy'] = df['Target_Taxonomy'].apply(map_target_taxonomy)\n",
    "df['map_target_organism'] = df['Target_organism'].apply(map_target_organism)\n",
    "\n",
    "# # TMAP layout and indexing\n",
    "# logger.info('Indexing...')\n",
    "# lf = tm.LSHForest(512, 128, store=True)\n",
    "\n",
    "# # Convert back to VectorUint if loading from file\n",
    "# if isinstance(fused_fingerprints[0], list):\n",
    "#     fused_fingerprints = [tm.VectorUint(fp) for fp in fused_fingerprints]\n",
    "\n",
    "# lf.batch_add(fused_fingerprints)\n",
    "# lf.index()\n",
    "\n",
    "def create_feature_vector(fingerprint, df_row, encoders):\n",
    "    \"\"\"\n",
    "    Create a feature vector for a single molecule.\n",
    "    \n",
    "    Args:\n",
    "    fingerprint (VectorUint): The 2048-bit fingerprint vector\n",
    "    df_row (pandas.Series): A row from the DataFrame containing other features\n",
    "    encoders (dict): Dictionary of fitted encoders for categorical variables\n",
    "    \n",
    "    Returns:\n",
    "    numpy.array: The concatenated feature vector\n",
    "    \"\"\"\n",
    "    # Convert fingerprint to numpy array\n",
    "    fp_array = np.array(fingerprint)\n",
    "    \n",
    "    # Normalize standard_value\n",
    "    # standard_value = encoders['standard_value'].transform([[df_row['standard_value']]])[0]\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    categorical_columns = ['mapped_protein_class', 'map_target_taxonomy', 'map_target_organism']\n",
    "    categorical_features = []\n",
    "    for col in categorical_columns:\n",
    "        # Create a DataFrame with the column name to avoid the warning\n",
    "        feature_df = pd.DataFrame({col: [df_row[col]]})\n",
    "        encoded = encoders[col].transform(feature_df)\n",
    "        categorical_features.append(encoded.toarray()[0])\n",
    "    \n",
    "    # Concatenate all features\n",
    "    return np.concatenate([fp_array] + categorical_features)\n",
    "\n",
    "def prepare_encoders(df):\n",
    "    \"\"\"\n",
    "    Prepare encoders for the dataset.\n",
    "    \n",
    "    Args:\n",
    "    df (pandas.DataFrame): The complete dataset\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary of fitted encoders\n",
    "    \"\"\"\n",
    "    encoders = {}\n",
    "    \n",
    "    # # StandardScaler for standard_value\n",
    "    # encoders['standard_value'] = StandardScaler().fit(df[['standard_value']])\n",
    "    \n",
    "    # OneHotEncoder for categorical variables\n",
    "    categorical_columns = ['mapped_protein_class', 'map_target_taxonomy', 'map_target_organism']\n",
    "    for col in categorical_columns:\n",
    "        encoders[col] = OneHotEncoder(sparse=True, handle_unknown='ignore').fit(df[[col]])\n",
    "    \n",
    "    return encoders\n",
    "\n",
    "def create_feature_vectors(fingerprints, df):\n",
    "    \"\"\"\n",
    "    Create feature vectors for all molecules in the dataset.\n",
    "    \n",
    "    Args:\n",
    "    fingerprints (list): List of fingerprint vectors\n",
    "    df (pandas.DataFrame): DataFrame containing other molecular features\n",
    "    \n",
    "    Returns:\n",
    "    list: List of feature vectors for all molecules\n",
    "    \"\"\"\n",
    "    encoders = prepare_encoders(df)\n",
    "    feature_vectors = []\n",
    "    \n",
    "    for fingerprint, (_, row) in zip(fingerprints, df.iterrows()):\n",
    "        feature_vector = create_feature_vector(fingerprint, row, encoders)\n",
    "        feature_vectors.append(feature_vector)\n",
    "    \n",
    "    return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors = create_feature_vectors(fingerprints, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([5.367026e+06, 5.910264e+06, 5.006740e+05, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00]),\n",
       " array([3.068239e+06, 5.684400e+05, 3.975866e+06, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00]),\n",
       " array([5.367026e+06, 5.910264e+06, 1.851512e+06, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00]),\n",
       " array([5.367026e+06, 5.910264e+06, 1.851512e+06, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00]),\n",
       " array([1.348362e+06, 5.910264e+06, 3.387800e+04, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00]),\n",
       " array([5.367026e+06, 5.910264e+06, 1.851512e+06, ..., 1.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00]),\n",
       " array([1.038064e+06, 4.915750e+05, 5.006740e+05, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00]),\n",
       " array([5.367026e+06, 5.910264e+06, 5.006740e+05, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00]),\n",
       " array([5367026., 5910264.,  500674., ...,       0.,       0.,       0.]),\n",
       " array([5367026., 5910264.,  500674., ...,       0.,       0.,       0.]),\n",
       " array([3252112.,  251111., 3364378., ...,       0.,       0.,       0.]),\n",
       " array([ 130251.,  251111., 3364378., ...,       0.,       0.,       0.]),\n",
       " array([2.676960e+05, 2.216366e+06, 2.940922e+06, ..., 1.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00]),\n",
       " array([3252112.,  251111., 3364378., ...,       0.,       0.,       0.]),\n",
       " array([2.676960e+05, 3.798263e+06, 2.940922e+06, ..., 1.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00]),\n",
       " array([6.182733e+06, 1.309861e+06, 4.749010e+05, ..., 0.000000e+00,\n",
       "        0.000000e+00, 1.000000e+00]),\n",
       " array([2.255200e+06, 2.459138e+06, 4.749010e+05, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00]),\n",
       " array([2.441395e+06, 3.201164e+06, 4.749010e+05, ..., 0.000000e+00,\n",
       "        1.000000e+00, 0.000000e+00]),\n",
       " array([4744455., 1309861., 4050690., ...,       0.,       0.,       0.]),\n",
       " array([   72581.,  1309861., 11406286., ...,        0.,        0.,\n",
       "               0.])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
